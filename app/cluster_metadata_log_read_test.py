import struct
import codecs
import math
from abc import ABC

TAG_BUFFER = b'\x00'
NULL_TOPIC_ID = bytes(16)
CLUSTER_METADATA_FILE = '/tmp/kraft-combined-logs/__cluster_metadata-0/00000000000000000000.log'


def advance_cursors(new_beginning:int, interval_length: int) -> tuple[int, int]:
    return (new_beginning, new_beginning + interval_length)

with codecs.open(r'C:\Users\cphil\Documents\projects\codecrafters\codecrafters-kafka-python\app\00000000000000000000.log', 'r', 'quopri_codec') as f:
    data = f.read()

print()

def decode_unsigned_beb128(byte_array: bytearray) -> tuple[int, int]:
    offset = 0
    end = False
    arr = []
    result = 0
    while not end:
        single_byte = byte_array[offset]
        # Check for continuing byte
        if single_byte & 0b10000000 == 0:
                # print(single_byte, 'Not a continuing byte')
                end = True
        # Remove the most significant bit
        single_byte = single_byte & 0b01111111
        arr.insert(0, single_byte)
        # Increment offset
        offset += 1
    result = 0
    for i in range(len(arr)):
        push_multiplier = (len(arr)-(i+1))
        elem = arr[i] << (7*push_multiplier)
        result |= elem
    return result, len(arr)

def decode_signed_beb128(byte_array: bytearray) -> tuple[int, int]:
    number, incr_amt = decode_unsigned_beb128(byte_array)
    if number % 2 == 0:
        return number // 2, incr_amt
    else:
        return -1 *( (number // 2) + 1), incr_amt
    
# def get_increment_amount(number: int) -> int:
#     num_bits = len(bin(number))-1
#     num_bytes = num_bits / 8
#     incr_number = int(math.ceil(num_bytes))
#     return incr_number

# with open('/tmp/kraft-combined-logs/__cluster_metadata-0/partition.metadata', 'r') as f:
#     print('partition.metadata file: ', f.read())
topics = {}
topic_names_by_uuid = {}
topics_metadata = {}
metadata_records = []
cur_end, batch_length = 0, 999
while data:
    cur_beg, cur_end = 0, 61
    # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_end])
    base_offset, batch_length, part_leader_epoch, magic_byte, crc, attrs, last_offset_delta, base_ts, max_ts, producer_id, producer_epoch, base_seq, records_length_count = struct.unpack('>qiibihiqqqhii', data[cur_beg:cur_end]) # type: ignore
    # print(list(zip(['base_offset', 'batch_length', 'part_leader_epoch', 'magic_byte', 'crc', 'attrs', 'last_offset_delta', 'base_ts', 'max_ts', 'producer_id', 'producer_epoch', 'base_seq', 'records_length_count'], struct.unpack('>qiibihiqqqhii', data[cur_beg:cur_end]))) ) # type: ignore
    cur_beg, cur_end = advance_cursors(cur_end, 1)
    # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_end])
    # length_of_record = struct.unpack('>b', data[cur_beg:cur_end])[0] # type: ignore
    for record in range(records_length_count):
        length_of_record, incr_amt = decode_signed_beb128(bytearray(data[cur_beg:])) # type: ignore
        # print('length_of_record:', length_of_record)
        cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, 1)
        # print(struct.unpack('>bbbb', data[cur_beg:cur_beg+4]))
        # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_end])
        record_attrs = struct.unpack('>b', data[cur_beg:cur_end]) # type: ignore
        cur_beg, cur_end = advance_cursors(cur_end, 1)
        ts_delta, incr_amt = decode_signed_beb128(bytearray(data[cur_beg:])) # type: ignore
        cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, 1)
        offset_delta, incr_amt = decode_signed_beb128(bytearray(data[cur_beg:])) # type: ignore
        cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, 1)
        # print(list(zip(['attrs', 'ts_delta', 'offset_delta'], struct.unpack('>bbb', data[cur_beg:cur_end])))) # type: ignore
        # cur_beg, cur_end = advance_cursors(cur_end, 1)
        # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_end])
        key_length, incr_amt = decode_signed_beb128(bytearray(data[cur_beg:]))  # type: ignore
        # print('key length:' , key_length)
        # print('Num Records: ', records_length_count)
        # print('Parsing New Record!')
        # Key Parsing
        # print('key_length ', key_length)
        if key_length > 0:
            cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, key_length)
            # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_end])
            key = struct.unpack(f'>{key_length}s', data[cur_beg:cur_end]) # type: ignore
            # print('key:', key)
        else:
            # print('Key was NULL')
            cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, 1)
        # cur_beg, cur_end = advance_cursors(cur_end, 1)
        # Value Parsing
        # print('DECODING THE VALUE!!!')
        value_length, incr_amt = decode_signed_beb128(data[cur_beg:]) # type: ignore
        # print('Value Length: ', value_length)
        # print(f'Advancing {get_increment_amount(value_length)} positions')
        # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_beg + value_length])
        cur_beg, cur_end = advance_cursors(cur_beg + incr_amt, 3)
        # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_beg + value_length])
        frame_version, record_type, version = struct.unpack('>3b', data[cur_beg:cur_end]) # type: ignore
        # print(list(zip(['frame_version', 'record_type', 'version'], struct.unpack('>3b', data[cur_beg:cur_end])))) # type: ignore
        cur_beg, cur_end = advance_cursors(cur_end, 1)
        if record_type == 12: # Feature Level Record
            # print('DECODING THE RECORD!!!')
            # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_beg + 10])
            name_length, incr_amt = decode_unsigned_beb128(bytearray(data[cur_beg:]))  # type: ignore
            # print('name_length: ', name_length)
            name_length -= 1
            cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, name_length+4)
            # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_end])
            name, feature_level, tagged_fields_count, headers_array_count = struct.unpack(f'>{name_length}shbb', data[cur_beg:cur_end]) # type: ignore
            # print(list(zip(['name', 'feature_level', 'tagged_fields_count', 'headers_array_count'], [name, feature_level, tagged_fields_count, headers_array_count])))
            metadata_records.append(name.decode())
        elif record_type == 2: # Topic Record
            # cur_beg, cur_end = advance_cursors(cur_end, 1)
            name_length, incr_amt = decode_unsigned_beb128(bytearray(data[cur_beg:]))  # type: ignore
            # print('name_length: ', name_length)
            name_length -= 1
            cur_beg, cur_end = advance_cursors(cur_end, name_length+18)
            # print('Cursors:', cur_beg, cur_end, data[cur_beg:cur_end])
            topic_name, topic_uuid, tagged_fields_count, headers_array_count = struct.unpack(f'>{name_length}s16sbb', data[cur_beg:cur_end]) # type: ignore
            topic_uuid = topic_uuid.hex()
            topic_name = topic_name.decode()
            if not topics.get(topic_name):
                topics[topic_name] = {
                    'id':topic_uuid,
                    'partitions': {}
                    } 
                topic_names_by_uuid[topic_uuid] = topic_name
            if not topics_metadata.get(topic_uuid):
                topics_metadata[topic_uuid] = {}
            # print('Topic Name: ', topic_name)
        elif record_type == 3: # Partition Record
            cur_beg, cur_end = advance_cursors(cur_beg, 20)
            partition_id, topic_uuid_bin = struct.unpack(f'>i16s', data[cur_beg:cur_end]) # type: ignore
            topic_uuid = topic_uuid_bin.hex()
            partition_dict = {}

            cur_beg, cur_end = advance_cursors(cur_end,1)
            replica_array_length, incr_amt = decode_unsigned_beb128(data[cur_beg:]) # type: ignore
            replica_array_length -= 1 # The actual length is 1 less than the number bc it's a compact array
            cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, 4*replica_array_length)
            replicas = struct.unpack(f'>{replica_array_length}i', data[cur_beg:cur_end]) # type: ignore

            cur_beg, cur_end = advance_cursors(cur_end, 1)
            in_sync_replica_array_length, incr_amt = decode_unsigned_beb128(data[cur_beg:]) # type: ignore
            in_sync_replica_array_length -= 1 # The actual length is 1 less than the number bc it's a compact array
            cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, 4*in_sync_replica_array_length)
            in_sync_replicas = struct.unpack(f'>{in_sync_replica_array_length}i', data[cur_beg:cur_end]) # type: ignore
            
            cur_beg, cur_end = advance_cursors(cur_end, 1)
            removing_replicas_array_length, incr_amt = decode_unsigned_beb128(data[cur_beg:]) # type: ignore
            removing_replicas_array_length -= 1 # The actual length is 1 less than the number bc it's a compact array
            cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, 4*removing_replicas_array_length)
            removing_replicas = struct.unpack(f'>{removing_replicas_array_length}i', data[cur_beg:cur_end]) # type: ignore

            cur_beg, cur_end = advance_cursors(cur_end, 1)
            adding_replica_array_length, incr_amt = decode_unsigned_beb128(data[cur_beg:]) # type: ignore
            adding_replica_array_length -= 1 # The actual length is 1 less than the number bc it's a compact array
            cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, 4*adding_replica_array_length)
            adding_replicas = struct.unpack(f'>{adding_replica_array_length}i', data[cur_beg:cur_end]) # type: ignore

            cur_beg, cur_end = advance_cursors(cur_end, 12)
            replica_leader_id, leader_epoch, partition_epoch = struct.unpack(f'>3i', data[cur_beg:cur_end]) # type: ignore

            cur_beg, cur_end = advance_cursors(cur_end, 1)
            directories_array_length, incr_amt = decode_unsigned_beb128(data[cur_beg:]) # type: ignore
            directories_array_length -= 1 # The actual length is 1 less than the number bc it's a compact array
            cur_beg, cur_end = advance_cursors(cur_beg+incr_amt, 16*directories_array_length)
            directories = struct.unpack(f'>{"16s"*directories_array_length}', data[cur_beg:cur_end]) # type: ignore
            cur_beg, cur_end = advance_cursors(cur_end, 2)
            tagged_fields_count, headers_array_count = struct.unpack(f'>bb', data[cur_beg:cur_end]) # type: ignore

            partition_dict['replicas'] = replicas
            partition_dict['in_sync_replicas'] = in_sync_replicas
            partition_dict['removing_replicas'] = removing_replicas
            partition_dict['adding_replicas'] = adding_replicas
            partition_dict['directories'] = directories
            partition_dict['replica_leader_id'] = replica_leader_id
            partition_dict['leader_epoch'] = leader_epoch
            partition_dict['partition_epoch'] = partition_epoch
            partition_dict['data'] = []
            topic_name = topic_names_by_uuid[topic_uuid]
            topics[topic_name]['partitions'][partition_id] = partition_dict
        else: 
            # print(f'Found unknown record type: {record_type}')
            exit()
        cur_beg, cur_end = advance_cursors(cur_end, 1)
    data = data[cur_beg:]
# print(metadata_records)
print(topics)

class RecordTypeFormat(ABC):
    pass

class FeatureLevelForamt(RecordTypeFormat):
    pass

class TopicRecordFormat(RecordTypeFormat):
    pass

class PartitionRecordFormat(RecordTypeFormat):
    pass

class BytesReader(ABC):
    FORMAT = 'b'
    SUBRECORD_FORMATS = {} # Type ID(int) : RecordTypeFormat
    def __init__(self, byte_stream):
        self.cur_beg = 0
        self.cur_end = 0

    def advance_cursors(self, new_beginning:int, interval_length: int) -> None:
        self.cur_beg = new_beginning
        self.cur_end = new_beginning + interval_length